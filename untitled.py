# -*- coding: utf-8 -*-
"""Untitled

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fJ9Gc7aOzmJoNY6_jcY7DsFintaP-xhe
"""

import torch
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_digits
import matplotlib.pyplot as plt
import torch.optim as optim


# Load the dataset
digits = load_digits()
X = digits.data
Y = digits.target

# Split the dataset into training and testing sets
X_train, X_other, Y_train, Y_other = train_test_split(X, Y, test_size=0.3, random_state=42)

# Convert numpy arrays to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
X_test_tensor = torch.tensor(X_other, dtype=torch.float32)
Y_train_tensor = torch.tensor(Y_train)
Y_test_tensor = torch.tensor(Y_other)

# One-hot encode the labels
num_classes = 10  # since MNIST digits go from 0 to 9
Y_train_one_hot = F.one_hot(Y_train_tensor, num_classes=num_classes)
Y_test_one_hot = F.one_hot(Y_test_tensor, num_classes=num_classes)

# Verify shapes of the tensors
print("Shapes of tensors:")
print("X_train_tensor:", X_train_tensor.shape)
print("X_test_tensor:", X_test_tensor.shape)
print("Y_train_one_hot:", Y_train_one_hot.shape)
print("Y_test_one_hot:", Y_test_one_hot.shape)

import torch
import torch.nn as nn


def softmax(x):
    exps = torch.exp(x - torch.max(x))
    sum_exps = torch.sum(exps, dim=1, keepdim=True)
    return exps / sum_exps


class SoftmaxClassifier(nn.Module):
    def __init__(self, n_feat, n_classes):
        super(SoftmaxClassifier, self).__init__()
        self.linear = nn.Linear(n_feat, n_classes)

    def forward(self, x):
        a = self.linear(x)
        p = softmax(a)
        return p

len_traindata = len(X_train_tensor)
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
Y_train_tensor = torch.tensor(Y_train, dtype=torch.int64)

class SimpleClassifier(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(SimpleClassifier, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)
        torch.nn.init.xavier_uniform_(self.linear.weight)
        self.linear.bias.data.fill_(0.01)

    def forward(self, x):
        x = self.linear(x)
        return F.log_softmax(x, dim=1)

# Initializing model, criterion, and optimizer
model = SimpleClassifier(input_dim=64, output_dim=10)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

# List to hold the losses
losses = []

# Training loop
for epoch in range(50):
    optimizer.zero_grad()
    outputs = model(X_train_tensor)
    loss = criterion(outputs, Y_train_tensor)
    loss.backward()
    optimizer.step()
    losses.append(loss.item())  # Save the loss for this epoch
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

print("Model training is finalized!")

# Plotting the loss curve
plt.figure(figsize=(10, 6))
plt.plot(range(1, 51), losses, marker='o', linestyle='-', color='b')
plt.title('Loss Curve over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

criterion = nn.CrossEntropyLoss()  # Re-initialize if not already defined

model.eval()  # Set the model to evaluation mode
test_losses = []
with torch.no_grad():  # No need to track gradients for evaluation
    for i in range(len(X_test_tensor)):
        x = X_test_tensor[i].unsqueeze(0)  # Add batch dimension
        y = Y_test_tensor[i].unsqueeze(0)  # Add batch dimension
        y_pred = model(x)
        loss = criterion(y_pred, y)
        test_losses.append(loss.item())

average_loss = sum(test_losses) / len(test_losses)
print(f'Average test loss: {average_loss}')